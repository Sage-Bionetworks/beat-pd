{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction and processing\n",
    "see here for API tutorial on selecting data\n",
    "http://docs.synapse.org/rest/org/sagebionetworks/repo/web/controller/TableExamples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle #to save files\n",
    "from scipy.stats import skew, kurtosis, pearsonr\n",
    "from scipy.signal import butter, welch, filtfilt, resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import nolds\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "syn=synapseclient.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper fcns - Feature extraction and clip generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_clips(rawdata,clipsize=30,overlap=0,interp=True):\n",
    "    \n",
    "    len_tol = 0.8   #% of the intended clipsize below which clip is not used (deprecated for now)\n",
    "    #reindex time (relative to start)\n",
    "    t = rawdata.Timestamp\n",
    "    t = t-t.iloc[0]\n",
    "    rawdata.Timestamp = t\n",
    "    #create clips data\n",
    "    deltat = np.median(np.diff(rawdata.Timestamp))\n",
    "    idx = np.arange(0,rawdata.Timestamp.iloc[-1],clipsize*(1-overlap))\n",
    "    clips = []\n",
    "    for i in idx:\n",
    "        c = rawdata[(rawdata.Timestamp>=i) & (rawdata.Timestamp<i+clipsize)]\n",
    "        if len(c) == 50*clipsize:\n",
    "            clips.append(c)\n",
    "        #resample clip if not enough samples are there\n",
    "        elif len(c)>50*clipsize*len_tol: #for now hard code clip size (enforce 250 samples)\n",
    "            t = c.Timestamp.values\n",
    "            tnew = np.linspace(t[0],t[-1],50*clipsize)  \n",
    "            x_res = resample(c.iloc[:,1].values,50*clipsize,t)[0]\n",
    "            y_res = resample(c.iloc[:,2].values,50*clipsize,t)[0]\n",
    "            z_res = resample(c.iloc[:,3].values,50*clipsize,t)[0]\n",
    "            mag_res = np.sqrt(x_res**2+y_res**2+z_res**2)\n",
    "            cres = pd.DataFrame(data=np.vstack((tnew,x_res,y_res,z_res,mag_res)).T,columns=\n",
    "                                ['Timestamp', 'X', 'Y', 'Z', 'Mag'])\n",
    "            clips.append(cres)\n",
    "    return clips\n",
    "\n",
    "def filterdata(rawdata,ftype='highpass',cutoff=0.75,cutoff_bp=[3,8],order=4):\n",
    "    \n",
    "    #takes rawdata as a parameter\n",
    "\n",
    "    if not rawdata.empty:\n",
    "        idx = rawdata.index\n",
    "        idx = idx-idx[0]\n",
    "        rawdata.index = idx\n",
    "        x = rawdata.values\n",
    "        #print(np.unique(np.diff(rawdata.index)))\n",
    "        Fs = np.mean(1/(np.diff(rawdata.index))) #sampling rate\n",
    "        if ftype != 'bandpass':\n",
    "            #filter design\n",
    "            cutoff_norm = cutoff/(0.5*Fs)\n",
    "            b,a = butter(4,cutoff_norm,btype=ftype,analog=False)\n",
    "        else:\n",
    "            #filter design\n",
    "            cutoff_low_norm = cutoff_bp[0]/(0.5*Fs)\n",
    "            cutoff_high_norm = cutoff_bp[1]/(0.5*Fs)\n",
    "            b,a = butter(order,[cutoff_low_norm,cutoff_high_norm],btype='bandpass',analog=False)\n",
    "\n",
    "        #filter data\n",
    "        xfilt = filtfilt(b,a,x,axis=0)\n",
    "        rawdatafilt = pd.DataFrame(data=xfilt,index=rawdata.index,columns=rawdata.columns)\n",
    "        return rawdatafilt\n",
    "\n",
    "def power_spectra_welch(rawdata,fm,fM):\n",
    "    #compute PSD on signal magnitude\n",
    "    x = rawdata.iloc[:,-1]\n",
    "    n = len(x) #number of samples in clip\n",
    "    Fs = np.mean(1/(np.diff(x.index))) #sampling rate in clip\n",
    "    f,Pxx_den = welch(x,Fs,nperseg=min(256,n))\n",
    "    #return PSD in desired interval of freq\n",
    "    inds = (f<=fM)&(f>=fm)\n",
    "    f=f[inds]\n",
    "    Pxx_den=Pxx_den[inds]\n",
    "    Pxxdf = pd.DataFrame(data=Pxx_den,index=f,columns=['PSD_magnitude'])\n",
    "\n",
    "    return Pxxdf\n",
    "\n",
    "\n",
    "def feature_extraction(rawdata):\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','Sen_X','Sen_Y','Sen_Z',\n",
    "                    'xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ','xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ',\n",
    "                    'Dom_freqX','Pdom_relX','PSD_meanX','PSD_stdX','PSD_skewX','PSD_kurX',\n",
    "                    'Dom_freqY','Pdom_relY','PSD_meanY','PSD_stdY','PSD_skewY','PSD_kurY',\n",
    "                    'Dom_freqZ','Pdom_relZ','PSD_meanZ','PSD_stdZ','PSD_skewZ','PSD_kurZ',\n",
    "                    'jerk_meanX','jerk_stdX','jerk_skewX','jerk_kurX',\n",
    "                    'jerk_meanY','jerk_stdY','jerk_skewY','jerk_kurY',\n",
    "                    'jerk_meanZ','jerk_stdZ','jerk_skewZ','jerk_kurZ',\n",
    "                    'RMS_mag','range_mag','mean_mag','var_mag','skew_mag','kurt_mag','Sen_mag',\n",
    "                    'Dom_freq_mag','Pdom_rel_mag','PSD_mean_mag','PSD_std_mag','PSD_skew_mag','PSD_kur_mag',\n",
    "                    'jerk_mean_mag','jerk_std_mag','jerk_skew_mag','jerk_kur_mag']\n",
    "\n",
    "    t = []\n",
    "    t1 = time.time()\n",
    "    rawdata_unfilt = rawdata.copy()\n",
    "\n",
    "    Fs = np.mean(1/(np.diff(rawdata.index)/1000)) #sampling rate\n",
    "\n",
    "    rawdata = filterdata(rawdata)\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) #append shared preprocessing time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    #acceleration magnitude\n",
    "    rawdata_wmag = rawdata_unfilt\n",
    "    rawdata_wmag['Accel_Mag']=np.sqrt((rawdata_unfilt**2).sum(axis=1))\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) #append magnitude computation time\n",
    "\n",
    "\n",
    "    #extract features on current clip\n",
    "\n",
    "    t1 = time.time()\n",
    "    #Root mean square of signal on each axis\n",
    "    N = len(rawdata)\n",
    "    RMS = 1/N*np.sqrt(np.asarray(np.sum(rawdata**2,axis=0)))\n",
    "\n",
    "    #range on each axis\n",
    "    min_xyz = np.min(rawdata,axis=0)\n",
    "    max_xyz = np.max(rawdata,axis=0)\n",
    "    r = np.asarray(max_xyz-min_xyz)\n",
    "\n",
    "    #Moments on each axis\n",
    "    mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "    var = np.asarray(np.std(rawdata,axis=0))\n",
    "    sk = skew(rawdata)\n",
    "    kurt = kurtosis(rawdata)\n",
    "\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append time domain features\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    #sample entropy raw data (magnitude) and FFT\n",
    "    sH_raw = []; #sH_fft = []\n",
    "\n",
    "    for a in range(3):\n",
    "        x = rawdata.iloc[:,a]\n",
    "        n = len(x) #number of samples in clip\n",
    "        Fs = np.mean(1/(np.diff(x.index))) #sampling rate in clip\n",
    "        sH_raw.append(nolds.sampen(x)) #samp entr raw data\n",
    "        #for now disable SH on fft\n",
    "        # f,Pxx_den = welch(x,Fs,nperseg=min(256,n/4))\n",
    "        # sH_fft.append(nolds.sampen(Pxx_den)) #samp entr fft\n",
    "\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append Sen features time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    #Cross-correlation between axes pairs\n",
    "    xcorr_xy = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,1],mode='same')\n",
    "    # xcorr_xy = xcorr_xy/np.abs(np.sum(xcorr_xy)) #normalize values\n",
    "    xcorr_peak_xy = np.max(xcorr_xy)\n",
    "    xcorr_lag_xy = (np.argmax(xcorr_xy))/len(xcorr_xy) #normalized lag\n",
    "\n",
    "    xcorr_xz = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,2],mode='same')\n",
    "    # xcorr_xz = xcorr_xz/np.abs(np.sum(xcorr_xz)) #normalize values\n",
    "    xcorr_peak_xz = np.max(xcorr_xz)\n",
    "    xcorr_lag_xz = (np.argmax(xcorr_xz))/len(xcorr_xz)\n",
    "\n",
    "    xcorr_yz = np.correlate(rawdata.iloc[:,1],rawdata.iloc[:,2],mode='same')\n",
    "    # xcorr_yz = xcorr_yz/np.abs(np.sum(xcorr_yz)) #normalize values\n",
    "    xcorr_peak_yz = np.max(xcorr_yz)\n",
    "    xcorr_lag_yz = (np.argmax(xcorr_yz))/len(xcorr_yz)\n",
    "\n",
    "    #pack xcorr features\n",
    "    xcorr_peak = np.array([xcorr_peak_xy,xcorr_peak_xz,xcorr_peak_yz])\n",
    "    xcorr_lag = np.array([xcorr_lag_xy,xcorr_lag_xz,xcorr_lag_yz])\n",
    "\n",
    "    t2=time.time()\n",
    "    t.append(t2-t1) # append xcorr computation time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    axes_F = np.array([])\n",
    "    for a in range(3):\n",
    "        x = rawdata.iloc[:,a]\n",
    "        n = len(x) #number of samples in clip\n",
    "        Fs = np.mean(1/(np.diff(x.index))) #sampling rate in clip\n",
    "        f,Pxx_den = welch(x,Fs,nperseg=min(256,n))\n",
    "        Pxx = pd.DataFrame(data=Pxx_den,index=f,columns=['PSD'])\n",
    "        F_rel = np.asarray([Pxx.iloc[Pxx.index<12,-1].idxmax()])\n",
    "        P_rel = Pxx.loc[F_rel].iloc[:,-1].values/Pxx.iloc[Pxx.index<12,-1].sum()\n",
    "        F_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values)[0],kurtosis(Pxx.values)[0]])\n",
    "        axes_F = np.concatenate((axes_F,F_rel,P_rel,F_moments))\n",
    "\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append frequency axes computation time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    #moments of jerk axes\n",
    "    axes_D = np.array([])\n",
    "    for a in range(3):\n",
    "        ax = rawdata.iloc[:,a].diff().values\n",
    "        ax_moments = np.array([np.nanmean(ax),np.nanstd(ax),skew(ax[~np.isnan(ax)]),kurtosis(ax[~np.isnan(ax)])])\n",
    "        axes_D = np.concatenate([axes_D,ax_moments])\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append axes derivative computation time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    RMS_mag = 1/N*np.sqrt(np.sum(rawdata_wmag['Accel_Mag']**2,axis=0))\n",
    "    r_mag = np.max(rawdata_wmag['Accel_Mag']) - np.min(rawdata_wmag['Accel_Mag'])\n",
    "    mean_mag = np.mean(rawdata_wmag['Accel_Mag'])\n",
    "    var_mag = np.std(rawdata_wmag['Accel_Mag'])\n",
    "    sk_mag = skew(rawdata_wmag['Accel_Mag'])\n",
    "    kurt_mag = kurtosis(rawdata_wmag['Accel_Mag'])\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append magnitude time domain computation time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    sH_mag = nolds.sampen(rawdata_wmag['Accel_Mag'])\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append magnitude entropy computation time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    #Dominant freq and relative magnitude (on acc magnitude)\n",
    "    Pxx = power_spectra_welch(rawdata_wmag,fm=0,fM=Fs)\n",
    "    domfreq = np.asarray([Pxx.iloc[Pxx.index<12,-1].idxmax()])\n",
    "    Pdom_rel = Pxx.loc[domfreq].iloc[:,-1].values/Pxx.iloc[Pxx.index<12,-1].sum() #power at dominant freq rel to total\n",
    "\n",
    "    #moments of PSD\n",
    "    Pxx_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values)[0],kurtosis(Pxx.values)[0]])\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append magnitude frequency computation time\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    #moments of jerk magnitude\n",
    "    jerk = rawdata_wmag['Accel_Mag'].diff().values\n",
    "    jerk_moments = np.array([np.nanmean(jerk),np.nanstd(jerk),skew(jerk[~np.isnan(jerk)]),kurtosis(jerk[~np.isnan(jerk)])])\n",
    "    t2 = time.time()\n",
    "    t.append(t2-t1) # append magnitude derivative computation time\n",
    "\n",
    "    #Assemble features in array\n",
    "    Y = np.array([RMS_mag,r_mag,mean_mag,var_mag,sk_mag,kurt_mag,sH_mag])\n",
    "    X = np.concatenate((RMS,r,mean,var,sk,kurt,sH_raw,xcorr_peak,xcorr_lag,axes_F,axes_D,Y,domfreq,Pdom_rel,Pxx_moments,jerk_moments))\n",
    "    \n",
    "    return X, features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntable = syn.tableQuery(\"SELECT * from syn20489608\")\n",
    "table = syntable.asDataFrame()\n",
    "table.to_csv('Metadata.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntable = syn.tableQuery(\"SELECT * from syn20489607\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = syn.downloadTableColumns(syntable,'smartwatch_accelerometer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = syntable.asDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['path']=table.smartwatch_accelerometer.astype(str).map(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.path.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in table.iterrows():\n",
    "    data = pd.read_csv(row[1].path)\n",
    "    clips = gen_clips(data)\n",
    "    # skip empty data\n",
    "    if len(clips)==0:\n",
    "        continue\n",
    "    DF_Recording = pd.DataFrame()\n",
    "    Fs=[]\n",
    "    for c in clips:\n",
    "        clip_edit = c.iloc[:,1:4].copy()\n",
    "        clip_edit.index=c['Timestamp']\n",
    "        F = feature_extraction(clip_edit)\n",
    "        Fs.append(pd.DataFrame(data=np.reshape(F[0], (1, -1)),columns=F[1],index=[0]))\n",
    "    \n",
    "    D = pd.concat(Fs)\n",
    "    D['ID'] = row[1].measurement_id\n",
    "    \n",
    "    D.to_csv(row[1].measurement_id+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = os.listdir('.\\\\Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.DataFrame()\n",
    "for f in Fs:\n",
    "    D_f = pd.read_csv('.\\\\Features\\\\'+f)\n",
    "    D = pd.concat([D,D_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.to_csv('.\\\\Features_Full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
